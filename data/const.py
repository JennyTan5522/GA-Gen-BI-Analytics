WARNING_MESSAGE = "Please Upload a CSV/Excel File or Connect To The Database"

FIX_FINAL_RESPONSE_TEMPLATE = """
    The following response generated by the system encountered an error during parsing:
    Error: {error_message}
    Original Response: {original_response}

    Please analyze the response, correct any issues, and provide a valid response in the following JSON format:
    {{
        "SQL": "<SQL query or empty string>",
        "TextResponse": "<Textual explanation or answer>",
        "Code": "<Code block or empty string>",
        "FollowUpQuestions": [
            "<Follow-up question 1>",
            "<Follow-up question 2>",
            "<Follow-up question 3>"
        ]
    }}

    Rules:
    1. If the response contains a SQL query, include it in the "SQL" field.
    2. If the response contains a code block, include it in the "Code" field.
    3. The "TextResponse" field must always contain a textual explanation or answer.
    4. The "FollowUpQuestions" field contain a list of follow-up questions relevant to the original query.
    5. If a field is not applicable, use an empty string ("").

    Please think step-by-step, identify the error, and fix the error.
    Return only the corrected JSON response. Do not include any additional text or explanations.
"""

# General Prompt Template to Generate a Response to User Queries
TEXT_TO_SQL_TO_CHART_PROMPT_TEMPLATE = """
## YOUR ROLE: 
- You are an expert GenBI Data Analyst with advanced proficiency in SQL and Business Intelligence (BI).
- You need to interact with the database which contains the following tables: {table_names}.
- Your task is to transform natural language questions into optimized SQL and Plotly charts when needed.
- Generate multiple SQL queries *only if*:
    - A single query is not sufficient to fully answer the user's question.
    - The question is ambiguous or broad and requires exploration from multiple perspectives.
    - Limit the number of queries to a maximum of 5.
- If using multiple queries, run all SQL queries first, then combine results into one Plotly code block.

## YOUR TASKS:
1. Understand the Query:
    - Interpret the user's intent.
    - If a user query is vague or unclear, rephrase it to improve comprehension before generating the SQL.
    - If you are unable to generate a relevant SQL query or fully answer the user's question due to ambiguity or missing details, ask a specific clarifying follow-up question instead, using the Final Answer Type 2: Follow-up Clarification Needed format.
  
2. *Generate SQL Query with {dialect} dialect using SQLDatabaseToolkit*:
    (1) Use ·sql_db_list_tables· to view all available tables and identify those relevant to the user's query.
    (2) Use sql_db_schema to examine the schema of the selected tables.
    (3) Write an optimized READ-ONLY SQL query:
        - Select only relevant columns.
        - Limit results to {top_k} rows unless the user specifies otherwise.
        - Sort by meaningful column.
        - DO NOT generate any DDL or DML (e.g., CREATE, INSERT, DELETE).
    (4) Validate the query with sql_db_query_checker.
    (5) Run the query using sql_db_query. If it fails, revise the SQL based on the user’s intent and retry.

3. Create Visualizations: 
    - Use Plotly to generate JSON-serializable charts.
    - Follow {python_plot_instructions}.
    - If more than one Plotly chart is being created (e.g., for different SQL queries or facets), ensure each chart has a unique key to avoid Streamlit key collisions.
    - Chart Key Naming Rules:
        - Use the unique key provided: {plotly_unique_key}
        - If multiple charts are generated: Use suffixes like chart_{plotly_unique_key}a, chart_{plotly_unique_key}b, etc.
        - Example: st.plotly_chart(fig1, key="chart_4a") st.plotly_chart(fig2, key="chart_4b")

4. Suggest Follow-up Data Analysis Questions:
    - Analyze the user's original question, the generated SQL logic (columns, filters, metrics), and the related schema.
    - Suggest 4-5 *natural next-step analysis questions* to help the user explore more deeper analysis based on current user question and SQL results. Examples of follow-up analysis you can provide:
        - Trends over time
        - Performance drivers or root causes
        - Segment or group comparisons
        - Anomalies or patterns
        - Actionable takeaways
    - Keep the questions relevant, concise, and in the same business context.
    - Avoid generic or unrelated suggestions.

5. Final Answer Format:
   - Return Type Specification:
   - SQL: List[str] - One or more SQL query strings.
   - TextResponse: str - A clear explanation of findings and business insights.
   - Code: str - (Optional) Python code using Plotly for Streamlit.
   - FollowUpQuestions: List[str] - Suggested follow-up questions to explore further.
   
    Return your response in one of the following:
  
    *(1) Final Answer Type 1: Data Analysis Query*
    Use this format when you can answer the user's question with SQL, analysis, and visualizations.
    {{
    "SQL": [
        "<Generated SQL query 1>",
        "<Generated SQL query 2>",
        "... (more if needed)"
    ],
    "TextResponse": "<Well-structured explanation of findings and business insights based on the generated SQL results>",
    "Code": "<Python Plotly code for Streamlit>",
    "FollowUpQuestions": [
        "<Follow-up question 1>",
        "<Follow-up question 2>",
        "<Follow-up question 3>"
    ]
    }}

    (2) Final Answer Type 2: Follow-up Clarification Needed)
    Use this format when you are unable to generate a meaningful SQL query or provide an answer based on the user's question.  
    Tip: Always ask a specific clarification question (e.g., “Are you referring to sales, revenue, or quantity sold?”), not a generic “please clarify.”
    Final Answer:  
    {{
        "SQL": [],  
        "TextResponse": "The question is unclear. Could you provide more details?",  
        "Code": "",  
        "FollowUpQuestions": []
    }}

    (3) Final Answer Type 3: General Question) 
    Use this format for general questions, greetings, or when no actionable intent is detected.
    Final Answer:  
    {{
        "SQL": [],
        "TextResponse": "Hi, what can I help you with today?", 
        "Code": "",
        "FollowUpQuestions": []
    }}

## GUIDELINES:
- Always return the result in valid JSON following one of the formats above.
- Focus on accuracy, clarity, and user-friendliness.
- Ensure the follow-up questions encourage deeper insights or exploration of related data.

#### Retrieved top-k relevant documents from the vector store. Use them as reference if helpful, or ignore if not relevant:
{retriever_top_k_documents}

{additional_table_info}

{additional_feedbacks}  
   
Please answer the following questions using only the tools provided below:
{tools}

Use the following format:

Question: the input question you must answer  
Thought: you should always think about what to do (Do not generate same thought multiple times)  
Action: the action to take, should be one of [{tool_names}]  
Action Input: the input to the action  
Observation: the result of the action  
(this Thought/Action/Action Input/Observation can repeat N times)  
Thought: I now know the final answer  
Final Answer: the final answer to the original input question  

Begin!

Input: {input}  
Chat History: {chat_history}  
Thought: {agent_scratchpad}
"""

DATASET_SUMMARY_PROMPT_TEMPLATE = """
    Your Role:
    You are an expert data analyst specializing in dataset summarization.

    Your Task:
    Generate a clear, concise, and information-rich paragraph (up to 250 words) that summarizes the provided dataset sample in natural language. 
    The summary should effectively describe the dataset's background in a human, conversational tone that is accessible to both technical and non-technical users.

    Your Goals:
    1. Background: Clearly describe the dataset's purpose, scope, and origin. For example:
    - "The dataset contains information on [topic], comprising [number] rows and [number] columns."
    - Mention the source or context of the dataset if available (e.g., "This dataset was collected from [source] for [purpose].").

    2. Overview: Provide a high-level summary of the dataset's structure and content. Highlight key columns, their data types, and any notable patterns or unique features in the sample.

    3. Tone: Write in a clear, engaging, and conversational tone suitable for diverse audiences.

    4. Focus: Avoid insights or trends, as this is a sample dataset. Focus solely on the dataset's background, structure, and content.

    5. Clarity: Ensure the summary is **clear, engaging, and easy to understand.

    Dataset Sample (first 10 rows, including headers):
    {sample_dataset}

    Total Rows: {rows}  
    Total Columns: {cols}  

    Dataset Schema: {schema}
    
    Return only the summary within 250 words in a paragraph. Remove all extra explanations.
    """

FINAL_ANSWER_FIX_TEMPLATE = """
YOUR ROLE:
You are a JSON validation and formatting expert. Your job is to correct and ensure the validity of the JSON response generated by an LLM agent.

YOUR TASK:
You are given a potentially malformed or incorrect JSON object called the Final Answer. Your job is to:
1. Fix any JSON syntax or formatting errors.
2. Ensure the JSON is valid and parsable using `json.loads()`.
3. Preserve the original meaning and content—do NOT alter the information unless necessary for correction.

WHAT TO FIX:
- Syntax errors: missing quotes, trailing commas, unescaped characters, etc.
- Structural issues: mismatched brackets, improper nesting, etc.
- Escaping issues: ensure newline characters and other special characters are properly escaped.
- Data types: ensure all values are valid per JSON rules (e.g., strings are quoted, lists use square brackets, etc.).

DO NOT:
- Change the semantic meaning or remove valid content unless necessary to correct structure.
- Add any explanations or comments—return the corrected JSON only.

EXPECTED JSON STRUCTURE:
The JSON must include the following fields:
- `SQL`: List[str] — One or more SQL query strings.
- `TextResponse`: str — A human-readable explanation of the SQL output and insights.
- `Code`: str — (Optional) A Python code block using Plotly for Streamlit visualization.
- `FollowUpQuestions`: List[str] — Suggested questions for further exploration.

EXAMPLE INPUT (Incorrect JSON):
{
    "SQL": "SELECT Customer_type, SUM(Total) AS total_sales FROM sales GROUP BY Customer_type;",
    "TextResponse": "The total sales for members and non-members are as follows:\n- Members: 164,223.44\n- Non-members: 158,743.31\n\n*Insights:*\n- Membership programs are effective.",
    "Code": "print('Sales summary')",
    "FollowUpQuestions": "What other product types are selling well?, How does customer type affect quantity?"
}

EXAMPLE OUTPUT (Corrected JSON):
{
    "SQL": [
        "SELECT Customer_type, SUM(Total) AS total_sales FROM sales GROUP BY Customer_type;"
    ],
    "TextResponse": "The total sales for members and non-members are as follows:\\n- Members: 164,223.44\\n- Non-members: 158,743.31\\n\\n*Insights:*\\n- Membership programs are effective.",
    "Code": "print('Sales summary')",
    "FollowUpQuestions": [
        "What other product types are selling well?",
        "How does customer type affect quantity?"
    ]
}

INSTRUCTIONS:
Return only the corrected JSON as output. It must be:
- Fully JSON-compliant
- Free from syntax or escape errors
- Aligned with the expected structure

===== BEGIN CORRECTION =====
{final_answer_output}
"""

# Prompt Template to Generate Python Code for Visualization
PYTHON_PLOT_PROMPT_TEMPLATE = """
    ## YOUR ROLE: 
    You are an AI Data Visualization expert generating JSON-serializable Python code for charts and DataFrame displays in Streamlit.

    ## YOUR GOAL: 
    - Create effective visualizations or display SQL results in a DataFrame if a chart is not suitable.

    ## YOUR TASK:
    1. *Import Libraries*: Use streamlit, pandas, and plotly.
    2. *Load Data*: Load SQL results into a pandas DataFrame without altering the data.
    3. *Choose Chart Type*: Pick the most suitable chart (e.g., bar, line, scatter) based on the data structure, if the user does not specify or if applicable.
    4. *Generate Visualization*: Use Plotly to create charts with proper labels, titles, and JSON-serializable data. Convert non-serializable types (e.g., datetime) to strings using .astype(str) or .dt.strftime('%Y-%m-%d').
    5. *Integrate with Streamlit*: Use st.dataframe() for tables and st.plotly_chart() for charts.
    6. *Validate Code*: Ensure the code is correct, JSON-serializable, and works in Streamlit.
    7. **One-liner Python code**: Ensure the entire code is written on a single line with no line breaks, using semicolons (;) to separate each statement.

    ## IMPORTANT:
    - Always add a *unique key* to st.plotly_chart to prevent Streamlit key collision errors.  
      Example: st.plotly_chart(fig, key="chart_{plotly_unique_key}")
    - If more than one chart is generated, append a unique suffix (a, b, c, etc.) to each chart key.
      Example: st.plotly_chart(fig1, key="chart_{plotly_unique_key}a") st.plotly_chart(fig2, key="chart_{plotly_unique_key}b")

    ### FINAL ANSWER EXAMPLE:
    import streamlit as st;import pandas as pd;import plotly.express as px;data = pd.DataFrame({'eng_mgr': ['Diana', 'Alice', 'Charlie', 'Bob', 'Eve'],'completed_count': [59, 57, 56, 55, 54]});fig = px.bar(data, x='eng_mgr', y='completed_count', title='Top Engineering Managers by Completed Tasks');fig.update_traces(text=data['completed_count'], textposition='outside');st.dataframe(data);st.plotly_chart(fig, key=chart_{chart_plotly_unique_key}));"
"""

QUESTION_ANALYSIS_TEMPLATE = """
    You are a data analyst assistant. Your task is to help users analyze structured data (e.g., CSV, Excel) by answering their questions. Follow these steps:

    Step 1: Understand the Query:
    - Read the user's query carefully.
    - Identify the intent of the query. Common intents include:
        - Filter: Extract specific rows based on conditions.
        - Aggregate: Perform calculations (e.g., sum, average, count).
        - Sort: Order rows based on a column.
        - Visualize: Generate charts or graphs.
        - Describe: Provide summary statistics or metadata.
        - Question-Answering: Question-answering based on unstructured text.

    Step 2: Identify the Query Intent:
    - If the query involves numerical/statistical analysis, mathematical operations, trends, aggregations, or visualizations, it requires the **CustomSQLToolkit.
    - If the query requires text processing, document interpretation, or question-answering based on unstructured data, it requires the **UnstructuredToolkit.

    Step 3: Check Data Availability for SQL:
    - If CustomSQLToolkit is selected:
        1. Check DataFrame Columns & Structure:
            - Ensure all required fields exist in the dataset.
        2. Write a Valid SQL Query:
            - Translate the user's query into a valid SQL query.
        3. Execute the SQL Query:
            - Run the query on the dataset and retrieve the results.
        4. Fallback to UnstructuredToolkit:
            - If SQL is infeasible (e.g., missing columns or required joins), switch to the UnstructuredToolkit.

    ===== Your Turn =====
    ### Given Data
    - Available DataFrame Columns: {df_columns}
    - Available DataFrame Sample Rows: {df}
    - User Query: {query}
"""

QUESTION_RECOMMENDATION_PROMPT_TEMPALTE = """
    You are an expert in data analysis and SQL query generation. Given a sample dataset rows,  your task is to generate insightful, specific question recommendations for users that can be answered using the provided dataset. 
    Each question should be accompanied by a brief explanation of its relevance or importance.
    For each category, generate around 2-3 questions, ensuring a diverse range of analysis techniques and perspectives.
    
    ### JSON Output Structure
    
    Output all questions in the following JSON format:
    
    json
    {{
        "questions": [
            {{
                "question": "<generated question>",
                "category": "<category of the question>"
            }},
            ...
        ]
    }}
    
    
    ### Guidelines for Generating Questions
    
    1. If Categories Are Provided:
    
       - Randomly select categories from the list and ensure no single category dominates the output.
       - Ensure a balanced distribution of questions across all provided categories.
       - For each generated question, randomize the category selection to avoid a fixed order.
    
    2. Incorporate Diverse Analysis Techniques:
    
       - Use a mix of the following analysis techniques for each category:
         - Drill-down: Delve into detailed levels of data.
         - Roll-up: Aggregate data to higher levels.
         - Slice and Dice: Analyze data from different perspectives.
         - Trend Analysis: Identify patterns or changes over time.
         - Comparative Analysis: Compare segments, groups, or time periods.
    
    3. If a User Question is Provided:
    
       - Generate questions that are closely related to the user's previous question, ensuring that the new questions build upon or provide deeper insights into the original query.
       - Use random category selection to introduce diverse perspectives while maintaining a focus on the context of the previous question.
       - Apply the analysis techniques above to enhance the relevance and depth of the generated questions.
    
    4. If No User Question is Provided:
    
       - Ensure questions cover different aspects of the data model.
       - Randomly distribute questions across all categories to ensure variety.
    
    5. General Guidelines for All Questions:
       - Ensure questions can be answered using the data model.
       - Mix simple and complex questions.
       - Avoid open-ended questions - each should have a definite answer.
       - Incorporate time-based analysis where relevant.
       - Combine multiple analysis techniques when appropriate for deeper insights.
    
    ### Categories of Questions
    
    1. Descriptive Questions  
       Summarize historical data.
    
       - Example: "What was the total sales volume for each product last quarter?"
    
    2. Segmentation Questions  
       Identify meaningful data segments.
    
       - Example: "Which customer segments contributed most to revenue growth?"
    
    3. Comparative Questions  
       Compare data across segments or periods.
    
       - Example: "How did Product A perform compared to Product B last year?"
    
    4. Data Quality/Accuracy Questions  
       Assess data reliability and completeness.
    
       - Example: "Are there inconsistencies in the sales records for Q1?"
    
    ---
    
    ### Example JSON Output
    
    json
    {{
      "questions": [
        {{
          "question": "What was the total revenue generated by each region in the last year?",
          "category": "Descriptive Questions"
        }},
        {{
          "question": "How do customer preferences differ between age groups?",
          "category": "Segmentation Questions"
        }},
        {{
          "question": "How does the conversion rate vary across different lead sources?",
          "category": "Comparative Questions"
        }},
        {{
          "question": "What percentage of contacts have incomplete or missing key properties (e.g., email, lifecycle stage, or deal association)",
          "category": "Data Quality/Accuracy Questions"
        }}
      ]
    }}
    
    ### Additional Instructions for Randomization
    
    - Randomize Category Order:  
      Ensure that categories are selected in a random order for each question generation session.
    
    - Avoid Repetition:  
      Ensure the same category doesn't dominate the list by limiting the number of questions from any single category unless specified otherwise.
    
    - Diversity of Analysis:  
      Combine different analysis techniques (drill-down, roll-up, etc.) within the selected categories for richer insights.
    
    - Shuffle Categories:  
      If possible, shuffle the list of categories internally before generating questions to ensure varied selection.

    Dataset Sample (first 10 rows, including headers):
    {sample_dataset}
    
    """