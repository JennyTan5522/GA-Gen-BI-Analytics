from typing import Any, Optional
from pydantic import BaseModel, Field

from langchain_core.tools import BaseTool
from langchain_core.language_models import BaseLanguageModel
from langchain_core.callbacks import (
    AsyncCallbackManagerForToolRun,
    CallbackManagerForToolRun,
)
from langchain_core.prompts import PromptTemplate
from langchain.chains.llm import LLMChain

from const import FINAL_ANSWER_FIX_TEMPLATE

class FinalAnswerValidatorInput(BaseModel):
    final_answer_output: str = Field(
        description="The final answer generated by the LLM agent that contains invalid JSON format and needs correction."
    )

# Create a Tool instance from your function
class FinalAnswerValidatorTool(BaseTool):
    """Tool for validating and correcting the JSON format of the final answer generated by the LLM agent."""

    name: str = "FinalAnswerValidatorTool"
    description: str = " This tool validates and corrects the JSON format of the final answer generated by the LLM agent. It ensures that the final answer follows proper JSON syntax, making it parsable by json.loads without changing the content."
    args_schema: type = FinalAnswerValidatorInput
    template: str = FINAL_ANSWER_FIX_TEMPLATE
    llm: BaseLanguageModel
    return_direct: bool = False

    def _init_(self, llm: BaseLanguageModel, **kwargs: Any) -> None:
        super()._init_(llm=llm, **kwargs)

    # run: synchronous method (executed in blocking method, program waits for the method to complete before moving on the next task)
    def _run(
        self, final_answer_output: str, run_manager: Optional[CallbackManagerForToolRun] = None
    ) -> str:
        """Use the tool."""
        prompt = PromptTemplate(template=self.template, input_variables=["final_answer_output"])
        chain = LLMChain(
                    llm=self.llm,
                    prompt=prompt
                )
        out = chain.predict(final_answer_output=final_answer_output, callbacks=run_manager.get_child() if run_manager else None)
        return out
    
    # arun: asynchronous method (executed in non-blocking method, program does not wait for the method to complete before moving on the next task - able to handle multiple tasks concurrently)
    async def _arun(
        self,
        final_answer_output: str,
        run_manager: Optional[AsyncCallbackManagerForToolRun] = None,
    ) -> str:
        """Use the tool asynchronously."""
        prompt = PromptTemplate(template=self.template, input_variables=["final_answer_output"])
        chain = LLMChain(
                    llm=self.llm,
                    prompt=prompt
                )
        out = await chain.apredict(final_answer_output=final_answer_output, callbacks=run_manager.get_child() if run_manager else None)
        return out